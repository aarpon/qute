<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>qute.data.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>qute.data.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#  ********************************************************************************
#   Copyright © 2022 - 2003, ETH Zurich, D-BSSE, Aaron Ponti
#   All rights reserved. This program and the accompanying materials
#   are made available under the terms of the Apache License Version 2.0
#   which accompanies this distribution, and is available at
#   https://www.apache.org/licenses/LICENSE-2.0.txt
#
#   Contributors:
#       Aaron Ponti - initial API and implementation
#  ******************************************************************************/
import json
import os
import re
import shutil
import sys
import time
from pathlib import Path
from typing import Optional, Union

import numpy as np
import userpaths
from imio import load, save
from natsort import natsorted
from numpy.random import default_rng


def sample(
    image: np.ndarray,
    patch_size: tuple,
    y0: Optional[int] = None,
    x0: Optional[int] = None,
    seed: Optional[int] = None,
) -&gt; tuple[np.ndarray, int, int]:
    &#34;&#34;&#34;Returns a (random) subset of given shape from the passed 2D image.

    Parameters
    ----------

    image: numpy array
        Original intensity image.

    patch_size: tuple
        Size (y, x) of the subset of the image to be randomly extracted.

    y0: Optional[int]
        y component of the top left corner of the extracted region.
        If omitted (default), it will be randomly generated.

    x0: Optional[int]
        x component of the top left corner of the extracted region.
        If omitted (default), it will be randomly generated.

    seed: Optional[int]
        Random generator seed to reproduce the sampling. Omit to create a
        new random sample every time.

    Returns
    -------

    result: tuple[np.ndarray, int, int]
        Subset of the image of given size; y coordinate of the top-left corner of
        the extracted subset; x coordinate of the top-left corner of the extracted subset.
    &#34;&#34;&#34;

    if image.ndim != 2:
        raise ValueError(&#34;The image must be 2D.&#34;)

    # Initialize random-number generator
    if seed is None:
        seed = time.time_ns()
    rng = np.random.default_rng(seed)

    # Get starting point
    max_y = image.shape[0] - patch_size[0]
    max_x = image.shape[1] - patch_size[1]
    if y0 is None:
        y0 = int(rng.uniform(0, max_y))
    if x0 is None:
        x0 = int(rng.uniform(0, max_x))

    # Return the subset and the starting coordinates
    return image[y0 : y0 + patch_size[0], x0 : x0 + patch_size[1]], y0, x0


def qute_to_msd_format(
    input_folder: Union[Path, str],
    output_folder: Union[Path, str],
    channel_names: tuple = (&#34;fluorescence_microscopy&#34;,),
    label_names: tuple = (&#34;background&#34;, &#34;cell&#34;, &#34;cell_border&#34;),
    dataset_id: int = 1,
    dataset_name: Optional[str] = None,
    stem_name: str = &#34;demo_&#34;,
    images_suffix: str = &#34;_0000&#34;,
    to_nii_gz: bool = False,
    test_perc: float = 0.2,
    num_folds: int = 5,
    force: bool = False,
    seed=None,
) -&gt; tuple[bool, str]:
    &#34;&#34;&#34;Convert a qute dataset (with sub-folders &#34;images&#34; and &#34;labels&#34;) to an MSD dataset.



    Parameters
    ----------

    input_folder: Union[Path, str]
        Full path of the dataset in qute format.

    output_folder: Union[Path, str]
        Full path of the nnUnetv2-compatible dataset. It is recommended to set this to `$nnNet_raw`.

    channel_names: tuple
        Name of the channels, e.g. (&#34;fluorescence_microscopy&#34;, )

    label_names: tuple
        Name of the segmentation classes, e.g. (&#34;background&#34;, &#34;cell&#34;, &#34;cell_border&#34;)

    dataset_id: int
        Integer ID of the dataset.

    dataset_name: str
        Name of the dataset. If not specified, it will be derived from `input_folder`.

    stem_name: str
        Stem name of all images. In contrast to the qute format, all training images, labels and test
        images must have the same stem name.

    images_suffix: str (default is &#34;_0000&#34;)
        Numeric suffix (as 0-padded string and leading _) for the images (it won&#39;t be applied to the labels).
        For instance, imagesTr/demo_001_0000.tif is matched by labelsTr/demo_001.tif.

    to_nii_gz: bool (default is False)
        Set to True to convert the TIFF images to nii.gz; otherwise, the TIFF files are just copied over.

    test_perc: float (default is 0.2)
        Fraction of the images to be used for testing.

    num_folds: int (default is 5)
        Number of folds for k-fold cross-validation during training.

    force: bool (default is False)
        Set to True to delete and recreate if a converted Dataset already exists in the `output_folder`, otherwise
        abort.

    seed: int (default is 2022)
        Seed for the random number generator (for training/test splitting).

    Returns
    -------

    res, msg: tuple(bool, str)
        res: True if the dataset was created, False otherwise
        msg: &#34;&#34; if successful; otherwise, contains error message.
    &#34;&#34;&#34;

    # Check the structure of the input folder
    input_folder = Path(input_folder)
    if not input_folder.is_dir():
        return False, f&#34;Input folder {input_folder} does not exit.&#34;

    # Check tha value of dataset_id
    if dataset_id != int(dataset_id) or dataset_id &lt; 1:
        raise ValueError(&#34;`dataset_id` must be a positive integer.&#34;)

    # Check the value of test_perc
    if test_perc &lt; 0.0 or test_perc &gt; 1.0:
        raise ValueError(&#34;`test_perc` must be between 0 and 1.&#34;)

    # Make sure the images and labels sub-folders exist
    images_folder = input_folder / &#34;images&#34;
    if not images_folder.is_dir():
        return False, f&#34;The `images` sub-folder does not exit.&#34;

    labels_folder = input_folder / &#34;labels&#34;
    if not labels_folder.is_dir():
        return False, f&#34;The `labels` sub-folder does not exit.&#34;

    # Get the contents of images_folder and labels_folder
    images_files = list(images_folder.glob(&#34;*.tif*&#34;))
    labels_files = list(labels_folder.glob(&#34;*.tif*&#34;))
    if len(images_files) == 0 or len(labels_files) == 0:
        return False, f&#34;No `images` or `labels` found.&#34;
    if len(images_files) != len(labels_files):
        return False, f&#34;Unmatched number of `images` and `labels`.&#34;

    # Check whether the output folder already exists, otherwise create
    output_folder = Path(output_folder)
    output_folder.mkdir(exist_ok=True)

    # Check whether the dataset folder already exists, otherwise create
    if dataset_name is None:
        dataset_name = input_folder.name
    dataset_folder = output_folder / f&#34;Dataset{dataset_id:03}_{dataset_name}&#34;
    dataset_folder.mkdir(exist_ok=True)

    # Check whether the training and test sub-folders already exist, otherwise create
    images_tr_folder = dataset_folder / &#34;imagesTr&#34;
    images_tr_folder.mkdir(exist_ok=True)
    labels_tr_folder = dataset_folder / &#34;labelsTr&#34;
    labels_tr_folder.mkdir(exist_ok=True)
    images_ts_folder = dataset_folder / &#34;imagesTs&#34;
    images_ts_folder.mkdir(exist_ok=True)

    # Check if the sub-folders already contain images
    empty = True
    if len(list(images_ts_folder.glob(&#34;*.tif*&#34;))) &gt; 0:
        empty = False
    if len(list(labels_tr_folder.glob(&#34;*.tif*&#34;))) &gt; 0:
        empty = False
    if len(list(images_ts_folder.glob(&#34;*.tif*&#34;))) &gt; 0:
        empty = False
    if not empty:
        if not force:
            return False, &#34;Dataset already existing. As requested, we abort here.&#34;

        # Delete sub-folders
        shutil.rmtree(images_tr_folder)
        shutil.rmtree(labels_tr_folder)
        shutil.rmtree(images_ts_folder)

        # Re-create them
        images_tr_folder.mkdir(exist_ok=True)
        labels_tr_folder.mkdir(exist_ok=True)
        images_ts_folder.mkdir(exist_ok=True)

    # Make sure that the filenames are sorted (naturally)
    images_files = natsorted(images_files)
    labels_files = natsorted(labels_files)

    # Shuffle a copy of the file names
    rng = default_rng(seed=seed)
    shuffled_indices = rng.permutation(len(images_files))
    shuffled_images_files = np.array(images_files.copy())[shuffled_indices].tolist()
    shuffled_labels_files = np.array(labels_files.copy())[shuffled_indices].tolist()

    # Split between training and testing sub-sets
    threshold = int(round((1.0 - test_perc) * len(images_files)))
    if threshold == 0 or threshold == len(images_files):
        return False, &#34;The requested `test_perc` fails splitting the images properly.&#34;
    selected_training_images_files = shuffled_images_files[:threshold]
    selected_training_labels_files = shuffled_labels_files[:threshold]
    selected_test_images_files = shuffled_images_files[threshold:]

    def target_name(convert: bool, name: str, stem: str, suffix: str = &#34;&#34;):
        &#34;&#34;&#34;Rename the target image to fit the nnUNetv2 expected pattern.&#34;&#34;&#34;
        numbers = re.findall(&#34;(\d+).tif$&#34;, Path(name).name)
        if len(numbers) != 1:
            return name
        if convert:
            new_name = f&#34;{stem}{numbers[0]}{suffix}.nii.gz&#34;
        else:
            new_name = f&#34;{stem}{numbers[0]}{suffix}.tif&#34;
        return new_name

    # Prepare output file names
    renamed_selected_training_images_files = []
    renamed_selected_training_labels_files = []
    renamed_selected_test_images_files = []
    for f in selected_training_images_files:
        renamed_selected_training_images_files.append(
            f&#34;{images_tr_folder}/{target_name(to_nii_gz, f, stem_name, images_suffix)}&#34;
        )
    for f in selected_training_labels_files:
        renamed_selected_training_labels_files.append(
            f&#34;{labels_tr_folder}/{target_name(to_nii_gz, f, stem_name)}&#34;
        )
    for f in selected_test_images_files:
        renamed_selected_test_images_files.append(
            f&#34;{images_ts_folder}/{target_name(to_nii_gz, f, stem_name, images_suffix)}&#34;
        )

    # Now copy or convert the files
    for f, o in zip(
        selected_training_images_files, renamed_selected_training_images_files
    ):
        if to_nii_gz:
            save.to_nii(load.load_any(f), o)
        else:
            shutil.copy(f, o)
    for f, o in zip(
        selected_training_labels_files, renamed_selected_training_labels_files
    ):
        if to_nii_gz:
            save.to_nii(load.load_any(f), o)
        else:
            shutil.copy(f, o)
    for f, o in zip(selected_test_images_files, renamed_selected_test_images_files):
        if to_nii_gz:
            save.to_nii(load.load_any(f), o)
        else:
            shutil.copy(f, o)

    # Create the datalist
    datalist = {
        # Add all test images to datalist[&#34;testing&#34;]
        &#34;testing&#34;: [
            {&#34;image&#34;: &#34;./imagesTs/&#34; + Path(file).name}
            for file in renamed_selected_test_images_files
        ],
        # Add all training images and labels as one fold
        &#34;training&#34;: [
            {
                &#34;image&#34;: f&#34;./imagesTr/{Path(renamed_selected_training_images_files[i]).name}&#34;,
                &#34;label&#34;: f&#34;./labelsTr/{Path(renamed_selected_training_labels_files[i]).name}&#34;,
                &#34;fold&#34;: 0,
            }
            for i in range(len(renamed_selected_training_images_files))
        ],
    }

    # Split training data into num_folds random folds
    fold_size = len(datalist[&#34;training&#34;]) // num_folds
    for i in range(num_folds):
        for j in range(fold_size):
            datalist[&#34;training&#34;][i * fold_size + j][&#34;fold&#34;] = i

    # Create dictionaries to store in the dataset.json file
    channel_names_dict = {}
    for i, channel in enumerate(channel_names):
        channel_names_dict[str(i)] = channel
    label_names_dict = {}
    for i, label in enumerate(label_names):
        label_names_dict[label] = i
    dataset = {
        &#34;channel_names&#34;: channel_names_dict,
        &#34;labels&#34;: label_names_dict,
        &#34;numTraining&#34;: len(datalist[&#34;training&#34;]),
        &#34;file_ending&#34;: &#34;.tif&#34;,
        &#34;dataset_name&#34;: f&#34;Dataset{dataset_id:03}_{dataset_name}&#34;,
    }
    with open(dataset_folder / &#34;dataset.json&#34;, &#34;w&#34;) as f:
        # write the dictionary to the file in JSON format
        json.dump(dataset, f)

    # Save the datalist
    datalist_file = (
        dataset_folder / f&#34;msd_{Path(dataset_folder).name.lower()}_folds.json&#34;
    )
    with open(datalist_file, &#34;w&#34;, encoding=&#34;utf-8&#34;) as f:
        json.dump(datalist, f, ensure_ascii=False, indent=4)

    # Finally, create input.yaml
    with open(dataset_folder / &#34;input.yaml&#34;, &#34;w&#34;) as f:
        f.write(f&#34;dataset_name_or_id: {dataset_id}\n&#34;)
        f.write(f&#34;modality: CT\n&#34;)
        f.write(f&#34;datalist: {datalist_file}\n&#34;)
        f.write(f&#34;dataroot: {dataset_folder}\n&#34;)

    # Inform
    print(f&#34;\n\n* * * All done! * * *&#34;)
    print(f&#34;\nData and configuration files written to `{dataset_folder}`.&#34;)
    print(
        f&#34;\nIf it is not already there, please make sure to copy/move `{dataset_folder.name}` to the folder &#34;
    )
    print(
        f&#34;pointed at by the environment variable `nnUNet_raw`. Then, assuming the dataset id is `1`\n&#34;
        f&#34;and the dataset is 2D, run the following (adapt accordingly): &#34;
    )
    print(f&#34;\n$ nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity&#34;)
    print(
        &#34;$ for i in {0..4}; do nnUNetv2_train 1 2d $i --npz; done   # Fold 0 through 4, adapt as necessary&#34;
    )
    print(f&#34;$ nnUNetv2_find_best_configuration -d 1 -f 0 1 2 3 4 -c 2d&#34;)
    print(
        f&#34;$ nnUNetv2_predict -d 1 -i $nnUNet_raw/$Dataset/imagesTs -o $nnUNet_results/$Dataset/inference -f 0 1 2 3 4 \\&#34;
    )
    print(f&#34;       -tr nnUNetTrainer -c 2d -p nnUNetPlans&#34;)
    print(
        f&#34;$ nnUNetv2_apply_postprocessing -i $nnUNet_results/$Dataset/inference -o $nnUNet_results/$Dataset/postprocessing \\&#34;
    )
    print(
        f&#34;       -pp_pkl_file $nnUNet_results/$Dataset/nnUNetTrainer__nnUNetPlans__2d/crossval_results_folds_0_1_2_3_4/postprocessing.pkl \\&#34;
    )
    print(
        f&#34;     -np 8 -plans_json $nnUNet_results/$Dataset/nnUNetTrainer__nnUNetPlans__2d/crossval_results_folds_0_1_2_3_4/plans.json&#34;
    )
    print(f&#34;\nIn the commands above, replace `$Dataset` with the dataset name.&#34;)
    print(f&#34;\nPlease see: &#34;)
    print(
        f&#34;  * https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/set_environment_variables.md&#34;
    )
    print(
        f&#34;  * https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/how_to_use_nnunet.md&#34;
    )
    print(
        f&#34;  * https://transformhealthcare.medium.com/glioblastoma-brain-tumor-segmentation-part-6-neural-network-model-training-5de238e9b195&#34;
    )
    print(
        f&#34;  * https://transformhealthcare.medium.com/glioblastoma-brain-tumor-segmentation-part-7-inference-58d4287a040d&#34;
    )

    # Return success
    return True, &#34;&#34;


if __name__ == &#34;__main__&#34;:

    # Path to qute demo segmentation dataset
    qute_dataset_folder = (
        Path(userpaths.get_my_documents())
        / &#34;qute&#34;
        / &#34;data&#34;
        / &#34;demo_segmentation_3_classes/&#34;
    )

    if &#34;nnUNet_raw&#34; in os.environ:
        nnUnet_raw_folder = Path(os.environ[&#34;nnUNet_raw&#34;])
    else:
        print(&#34;The environment variable `nnUNet_raw` is not defined.&#34;)
        sys.exit(1)

    qute_to_msd_format(
        input_folder=qute_dataset_folder,
        output_folder=nnUnet_raw_folder,
        force=True,
    )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="qute.data.utils.qute_to_msd_format"><code class="name flex">
<span>def <span class="ident">qute_to_msd_format</span></span>(<span>input_folder: Union[pathlib.Path, str], output_folder: Union[pathlib.Path, str], channel_names: tuple = ('fluorescence_microscopy',), label_names: tuple = ('background', 'cell', 'cell_border'), dataset_id: int = 1, dataset_name: Optional[str] = None, stem_name: str = 'demo_', images_suffix: str = '_0000', to_nii_gz: bool = False, test_perc: float = 0.2, num_folds: int = 5, force: bool = False, seed=None) ‑> tuple[bool, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a qute dataset (with sub-folders "images" and "labels") to an MSD dataset.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_folder</code></strong> :&ensp;<code>Union[Path, str]</code></dt>
<dd>Full path of the dataset in qute format.</dd>
<dt><strong><code>output_folder</code></strong> :&ensp;<code>Union[Path, str]</code></dt>
<dd>Full path of the nnUnetv2-compatible dataset. It is recommended to set this to <code>$nnNet_raw</code>.</dd>
<dt><strong><code>channel_names</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Name of the channels, e.g. ("fluorescence_microscopy", )</dd>
<dt><strong><code>label_names</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Name of the segmentation classes, e.g. ("background", "cell", "cell_border")</dd>
<dt><strong><code>dataset_id</code></strong> :&ensp;<code>int</code></dt>
<dd>Integer ID of the dataset.</dd>
<dt><strong><code>dataset_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the dataset. If not specified, it will be derived from <code>input_folder</code>.</dd>
<dt><strong><code>stem_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Stem name of all images. In contrast to the qute format, all training images, labels and test
images must have the same stem name.</dd>
<dt><strong><code>images_suffix</code></strong> :&ensp;<code>str (default is "_0000")</code></dt>
<dd>Numeric suffix (as 0-padded string and leading _) for the images (it won't be applied to the labels).
For instance, imagesTr/demo_001_0000.tif is matched by labelsTr/demo_001.tif.</dd>
<dt><strong><code>to_nii_gz</code></strong> :&ensp;<code>bool (default is False)</code></dt>
<dd>Set to True to convert the TIFF images to nii.gz; otherwise, the TIFF files are just copied over.</dd>
<dt><strong><code>test_perc</code></strong> :&ensp;<code>float (default is 0.2)</code></dt>
<dd>Fraction of the images to be used for testing.</dd>
<dt><strong><code>num_folds</code></strong> :&ensp;<code>int (default is 5)</code></dt>
<dd>Number of folds for k-fold cross-validation during training.</dd>
<dt><strong><code>force</code></strong> :&ensp;<code>bool (default is False)</code></dt>
<dd>Set to True to delete and recreate if a converted Dataset already exists in the <code>output_folder</code>, otherwise
abort.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int (default is 2022)</code></dt>
<dd>Seed for the random number generator (for training/test splitting).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>res</code></strong>, <strong><code>msg</code></strong> :&ensp;<code>tuple(bool, str)</code></dt>
<dd>res: True if the dataset was created, False otherwise
msg: "" if successful; otherwise, contains error message.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def qute_to_msd_format(
    input_folder: Union[Path, str],
    output_folder: Union[Path, str],
    channel_names: tuple = (&#34;fluorescence_microscopy&#34;,),
    label_names: tuple = (&#34;background&#34;, &#34;cell&#34;, &#34;cell_border&#34;),
    dataset_id: int = 1,
    dataset_name: Optional[str] = None,
    stem_name: str = &#34;demo_&#34;,
    images_suffix: str = &#34;_0000&#34;,
    to_nii_gz: bool = False,
    test_perc: float = 0.2,
    num_folds: int = 5,
    force: bool = False,
    seed=None,
) -&gt; tuple[bool, str]:
    &#34;&#34;&#34;Convert a qute dataset (with sub-folders &#34;images&#34; and &#34;labels&#34;) to an MSD dataset.



    Parameters
    ----------

    input_folder: Union[Path, str]
        Full path of the dataset in qute format.

    output_folder: Union[Path, str]
        Full path of the nnUnetv2-compatible dataset. It is recommended to set this to `$nnNet_raw`.

    channel_names: tuple
        Name of the channels, e.g. (&#34;fluorescence_microscopy&#34;, )

    label_names: tuple
        Name of the segmentation classes, e.g. (&#34;background&#34;, &#34;cell&#34;, &#34;cell_border&#34;)

    dataset_id: int
        Integer ID of the dataset.

    dataset_name: str
        Name of the dataset. If not specified, it will be derived from `input_folder`.

    stem_name: str
        Stem name of all images. In contrast to the qute format, all training images, labels and test
        images must have the same stem name.

    images_suffix: str (default is &#34;_0000&#34;)
        Numeric suffix (as 0-padded string and leading _) for the images (it won&#39;t be applied to the labels).
        For instance, imagesTr/demo_001_0000.tif is matched by labelsTr/demo_001.tif.

    to_nii_gz: bool (default is False)
        Set to True to convert the TIFF images to nii.gz; otherwise, the TIFF files are just copied over.

    test_perc: float (default is 0.2)
        Fraction of the images to be used for testing.

    num_folds: int (default is 5)
        Number of folds for k-fold cross-validation during training.

    force: bool (default is False)
        Set to True to delete and recreate if a converted Dataset already exists in the `output_folder`, otherwise
        abort.

    seed: int (default is 2022)
        Seed for the random number generator (for training/test splitting).

    Returns
    -------

    res, msg: tuple(bool, str)
        res: True if the dataset was created, False otherwise
        msg: &#34;&#34; if successful; otherwise, contains error message.
    &#34;&#34;&#34;

    # Check the structure of the input folder
    input_folder = Path(input_folder)
    if not input_folder.is_dir():
        return False, f&#34;Input folder {input_folder} does not exit.&#34;

    # Check tha value of dataset_id
    if dataset_id != int(dataset_id) or dataset_id &lt; 1:
        raise ValueError(&#34;`dataset_id` must be a positive integer.&#34;)

    # Check the value of test_perc
    if test_perc &lt; 0.0 or test_perc &gt; 1.0:
        raise ValueError(&#34;`test_perc` must be between 0 and 1.&#34;)

    # Make sure the images and labels sub-folders exist
    images_folder = input_folder / &#34;images&#34;
    if not images_folder.is_dir():
        return False, f&#34;The `images` sub-folder does not exit.&#34;

    labels_folder = input_folder / &#34;labels&#34;
    if not labels_folder.is_dir():
        return False, f&#34;The `labels` sub-folder does not exit.&#34;

    # Get the contents of images_folder and labels_folder
    images_files = list(images_folder.glob(&#34;*.tif*&#34;))
    labels_files = list(labels_folder.glob(&#34;*.tif*&#34;))
    if len(images_files) == 0 or len(labels_files) == 0:
        return False, f&#34;No `images` or `labels` found.&#34;
    if len(images_files) != len(labels_files):
        return False, f&#34;Unmatched number of `images` and `labels`.&#34;

    # Check whether the output folder already exists, otherwise create
    output_folder = Path(output_folder)
    output_folder.mkdir(exist_ok=True)

    # Check whether the dataset folder already exists, otherwise create
    if dataset_name is None:
        dataset_name = input_folder.name
    dataset_folder = output_folder / f&#34;Dataset{dataset_id:03}_{dataset_name}&#34;
    dataset_folder.mkdir(exist_ok=True)

    # Check whether the training and test sub-folders already exist, otherwise create
    images_tr_folder = dataset_folder / &#34;imagesTr&#34;
    images_tr_folder.mkdir(exist_ok=True)
    labels_tr_folder = dataset_folder / &#34;labelsTr&#34;
    labels_tr_folder.mkdir(exist_ok=True)
    images_ts_folder = dataset_folder / &#34;imagesTs&#34;
    images_ts_folder.mkdir(exist_ok=True)

    # Check if the sub-folders already contain images
    empty = True
    if len(list(images_ts_folder.glob(&#34;*.tif*&#34;))) &gt; 0:
        empty = False
    if len(list(labels_tr_folder.glob(&#34;*.tif*&#34;))) &gt; 0:
        empty = False
    if len(list(images_ts_folder.glob(&#34;*.tif*&#34;))) &gt; 0:
        empty = False
    if not empty:
        if not force:
            return False, &#34;Dataset already existing. As requested, we abort here.&#34;

        # Delete sub-folders
        shutil.rmtree(images_tr_folder)
        shutil.rmtree(labels_tr_folder)
        shutil.rmtree(images_ts_folder)

        # Re-create them
        images_tr_folder.mkdir(exist_ok=True)
        labels_tr_folder.mkdir(exist_ok=True)
        images_ts_folder.mkdir(exist_ok=True)

    # Make sure that the filenames are sorted (naturally)
    images_files = natsorted(images_files)
    labels_files = natsorted(labels_files)

    # Shuffle a copy of the file names
    rng = default_rng(seed=seed)
    shuffled_indices = rng.permutation(len(images_files))
    shuffled_images_files = np.array(images_files.copy())[shuffled_indices].tolist()
    shuffled_labels_files = np.array(labels_files.copy())[shuffled_indices].tolist()

    # Split between training and testing sub-sets
    threshold = int(round((1.0 - test_perc) * len(images_files)))
    if threshold == 0 or threshold == len(images_files):
        return False, &#34;The requested `test_perc` fails splitting the images properly.&#34;
    selected_training_images_files = shuffled_images_files[:threshold]
    selected_training_labels_files = shuffled_labels_files[:threshold]
    selected_test_images_files = shuffled_images_files[threshold:]

    def target_name(convert: bool, name: str, stem: str, suffix: str = &#34;&#34;):
        &#34;&#34;&#34;Rename the target image to fit the nnUNetv2 expected pattern.&#34;&#34;&#34;
        numbers = re.findall(&#34;(\d+).tif$&#34;, Path(name).name)
        if len(numbers) != 1:
            return name
        if convert:
            new_name = f&#34;{stem}{numbers[0]}{suffix}.nii.gz&#34;
        else:
            new_name = f&#34;{stem}{numbers[0]}{suffix}.tif&#34;
        return new_name

    # Prepare output file names
    renamed_selected_training_images_files = []
    renamed_selected_training_labels_files = []
    renamed_selected_test_images_files = []
    for f in selected_training_images_files:
        renamed_selected_training_images_files.append(
            f&#34;{images_tr_folder}/{target_name(to_nii_gz, f, stem_name, images_suffix)}&#34;
        )
    for f in selected_training_labels_files:
        renamed_selected_training_labels_files.append(
            f&#34;{labels_tr_folder}/{target_name(to_nii_gz, f, stem_name)}&#34;
        )
    for f in selected_test_images_files:
        renamed_selected_test_images_files.append(
            f&#34;{images_ts_folder}/{target_name(to_nii_gz, f, stem_name, images_suffix)}&#34;
        )

    # Now copy or convert the files
    for f, o in zip(
        selected_training_images_files, renamed_selected_training_images_files
    ):
        if to_nii_gz:
            save.to_nii(load.load_any(f), o)
        else:
            shutil.copy(f, o)
    for f, o in zip(
        selected_training_labels_files, renamed_selected_training_labels_files
    ):
        if to_nii_gz:
            save.to_nii(load.load_any(f), o)
        else:
            shutil.copy(f, o)
    for f, o in zip(selected_test_images_files, renamed_selected_test_images_files):
        if to_nii_gz:
            save.to_nii(load.load_any(f), o)
        else:
            shutil.copy(f, o)

    # Create the datalist
    datalist = {
        # Add all test images to datalist[&#34;testing&#34;]
        &#34;testing&#34;: [
            {&#34;image&#34;: &#34;./imagesTs/&#34; + Path(file).name}
            for file in renamed_selected_test_images_files
        ],
        # Add all training images and labels as one fold
        &#34;training&#34;: [
            {
                &#34;image&#34;: f&#34;./imagesTr/{Path(renamed_selected_training_images_files[i]).name}&#34;,
                &#34;label&#34;: f&#34;./labelsTr/{Path(renamed_selected_training_labels_files[i]).name}&#34;,
                &#34;fold&#34;: 0,
            }
            for i in range(len(renamed_selected_training_images_files))
        ],
    }

    # Split training data into num_folds random folds
    fold_size = len(datalist[&#34;training&#34;]) // num_folds
    for i in range(num_folds):
        for j in range(fold_size):
            datalist[&#34;training&#34;][i * fold_size + j][&#34;fold&#34;] = i

    # Create dictionaries to store in the dataset.json file
    channel_names_dict = {}
    for i, channel in enumerate(channel_names):
        channel_names_dict[str(i)] = channel
    label_names_dict = {}
    for i, label in enumerate(label_names):
        label_names_dict[label] = i
    dataset = {
        &#34;channel_names&#34;: channel_names_dict,
        &#34;labels&#34;: label_names_dict,
        &#34;numTraining&#34;: len(datalist[&#34;training&#34;]),
        &#34;file_ending&#34;: &#34;.tif&#34;,
        &#34;dataset_name&#34;: f&#34;Dataset{dataset_id:03}_{dataset_name}&#34;,
    }
    with open(dataset_folder / &#34;dataset.json&#34;, &#34;w&#34;) as f:
        # write the dictionary to the file in JSON format
        json.dump(dataset, f)

    # Save the datalist
    datalist_file = (
        dataset_folder / f&#34;msd_{Path(dataset_folder).name.lower()}_folds.json&#34;
    )
    with open(datalist_file, &#34;w&#34;, encoding=&#34;utf-8&#34;) as f:
        json.dump(datalist, f, ensure_ascii=False, indent=4)

    # Finally, create input.yaml
    with open(dataset_folder / &#34;input.yaml&#34;, &#34;w&#34;) as f:
        f.write(f&#34;dataset_name_or_id: {dataset_id}\n&#34;)
        f.write(f&#34;modality: CT\n&#34;)
        f.write(f&#34;datalist: {datalist_file}\n&#34;)
        f.write(f&#34;dataroot: {dataset_folder}\n&#34;)

    # Inform
    print(f&#34;\n\n* * * All done! * * *&#34;)
    print(f&#34;\nData and configuration files written to `{dataset_folder}`.&#34;)
    print(
        f&#34;\nIf it is not already there, please make sure to copy/move `{dataset_folder.name}` to the folder &#34;
    )
    print(
        f&#34;pointed at by the environment variable `nnUNet_raw`. Then, assuming the dataset id is `1`\n&#34;
        f&#34;and the dataset is 2D, run the following (adapt accordingly): &#34;
    )
    print(f&#34;\n$ nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity&#34;)
    print(
        &#34;$ for i in {0..4}; do nnUNetv2_train 1 2d $i --npz; done   # Fold 0 through 4, adapt as necessary&#34;
    )
    print(f&#34;$ nnUNetv2_find_best_configuration -d 1 -f 0 1 2 3 4 -c 2d&#34;)
    print(
        f&#34;$ nnUNetv2_predict -d 1 -i $nnUNet_raw/$Dataset/imagesTs -o $nnUNet_results/$Dataset/inference -f 0 1 2 3 4 \\&#34;
    )
    print(f&#34;       -tr nnUNetTrainer -c 2d -p nnUNetPlans&#34;)
    print(
        f&#34;$ nnUNetv2_apply_postprocessing -i $nnUNet_results/$Dataset/inference -o $nnUNet_results/$Dataset/postprocessing \\&#34;
    )
    print(
        f&#34;       -pp_pkl_file $nnUNet_results/$Dataset/nnUNetTrainer__nnUNetPlans__2d/crossval_results_folds_0_1_2_3_4/postprocessing.pkl \\&#34;
    )
    print(
        f&#34;     -np 8 -plans_json $nnUNet_results/$Dataset/nnUNetTrainer__nnUNetPlans__2d/crossval_results_folds_0_1_2_3_4/plans.json&#34;
    )
    print(f&#34;\nIn the commands above, replace `$Dataset` with the dataset name.&#34;)
    print(f&#34;\nPlease see: &#34;)
    print(
        f&#34;  * https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/set_environment_variables.md&#34;
    )
    print(
        f&#34;  * https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/how_to_use_nnunet.md&#34;
    )
    print(
        f&#34;  * https://transformhealthcare.medium.com/glioblastoma-brain-tumor-segmentation-part-6-neural-network-model-training-5de238e9b195&#34;
    )
    print(
        f&#34;  * https://transformhealthcare.medium.com/glioblastoma-brain-tumor-segmentation-part-7-inference-58d4287a040d&#34;
    )

    # Return success
    return True, &#34;&#34;</code></pre>
</details>
</dd>
<dt id="qute.data.utils.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>image: numpy.ndarray, patch_size: tuple, y0: Optional[int] = None, x0: Optional[int] = None, seed: Optional[int] = None) ‑> tuple[numpy.ndarray, int, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a (random) subset of given shape from the passed 2D image.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Original intensity image.</dd>
<dt><strong><code>patch_size</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Size (y, x) of the subset of the image to be randomly extracted.</dd>
<dt><strong><code>y0</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>y component of the top left corner of the extracted region.
If omitted (default), it will be randomly generated.</dd>
<dt><strong><code>x0</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>x component of the top left corner of the extracted region.
If omitted (default), it will be randomly generated.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Random generator seed to reproduce the sampling. Omit to create a
new random sample every time.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code>tuple[np.ndarray, int, int]</code></dt>
<dd>Subset of the image of given size; y coordinate of the top-left corner of
the extracted subset; x coordinate of the top-left corner of the extracted subset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(
    image: np.ndarray,
    patch_size: tuple,
    y0: Optional[int] = None,
    x0: Optional[int] = None,
    seed: Optional[int] = None,
) -&gt; tuple[np.ndarray, int, int]:
    &#34;&#34;&#34;Returns a (random) subset of given shape from the passed 2D image.

    Parameters
    ----------

    image: numpy array
        Original intensity image.

    patch_size: tuple
        Size (y, x) of the subset of the image to be randomly extracted.

    y0: Optional[int]
        y component of the top left corner of the extracted region.
        If omitted (default), it will be randomly generated.

    x0: Optional[int]
        x component of the top left corner of the extracted region.
        If omitted (default), it will be randomly generated.

    seed: Optional[int]
        Random generator seed to reproduce the sampling. Omit to create a
        new random sample every time.

    Returns
    -------

    result: tuple[np.ndarray, int, int]
        Subset of the image of given size; y coordinate of the top-left corner of
        the extracted subset; x coordinate of the top-left corner of the extracted subset.
    &#34;&#34;&#34;

    if image.ndim != 2:
        raise ValueError(&#34;The image must be 2D.&#34;)

    # Initialize random-number generator
    if seed is None:
        seed = time.time_ns()
    rng = np.random.default_rng(seed)

    # Get starting point
    max_y = image.shape[0] - patch_size[0]
    max_x = image.shape[1] - patch_size[1]
    if y0 is None:
        y0 = int(rng.uniform(0, max_y))
    if x0 is None:
        x0 = int(rng.uniform(0, max_x))

    # Return the subset and the starting coordinates
    return image[y0 : y0 + patch_size[0], x0 : x0 + patch_size[1]], y0, x0</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="qute.data" href="index.html">qute.data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="qute.data.utils.qute_to_msd_format" href="#qute.data.utils.qute_to_msd_format">qute_to_msd_format</a></code></li>
<li><code><a title="qute.data.utils.sample" href="#qute.data.utils.sample">sample</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
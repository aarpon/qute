<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>qute.models.unet API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>qute.models.unet</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#  ********************************************************************************
#   Copyright © 2022 - 2003, ETH Zurich, D-BSSE, Aaron Ponti
#   All rights reserved. This program and the accompanying materials
#   are made available under the terms of the Apache License Version 2.0
#   which accompanies this distribution, and is available at
#   https://www.apache.org/licenses/LICENSE-2.0.txt
#
#   Contributors:
#       Aaron Ponti - initial API and implementation
#
#   This file adapted from: https://github.com/hiepph/unet-lightning
#  ******************************************************************************/

from pathlib import Path
from typing import Tuple, Union

import pytorch_lightning as pl
import torch
from monai.data import DataLoader
from monai.inferers import sliding_window_inference
from monai.losses import DiceCELoss
from monai.metrics import DiceMetric
from monai.networks.nets import UNet as MonaiUNet
from monai.transforms import Transform
from tifffile import TiffWriter
from torch.optim import AdamW

import qute


class UNet(pl.LightningModule):
    &#34;&#34;&#34;Wrap MONAI&#39;s UNet architecture into a PyTorch Lightning module.

    The default settings are compatible with a classification task, where
    a single-channel input image is transformed into a three-class label image.
    &#34;&#34;&#34;

    def __init__(
        self,
        spatial_dims: int = 2,
        in_channels: int = 1,
        out_channels: int = 3,
        channels=(16, 32, 64),
        strides=(2, 2),
        criterion=DiceCELoss(include_background=False, to_onehot_y=False, softmax=True),
        metrics=DiceMetric(
            include_background=False, reduction=&#34;mean&#34;, get_not_nans=False
        ),
        val_metrics_transforms=None,
        predict_post_transforms=None,
        learning_rate: float = 1e-2,
        optimizer_class=AdamW,
        num_res_units: int = 0,
        dropout: float = 0.0,
    ):
        &#34;&#34;&#34;
        Constructor.

        Parameters
        ----------

        spatial_dims: int = 2
            Whether 2D or 3D data.

        in_channels: int = 1
            Number of input channels.

        out_channels: int = 3
            Number of output channels (or labels, or classes)

        channels: tuple = (16, 32, 64)
            Number of neuron per layer.

        strides: tuple = (2, 2)
            Strides for down-sampling.

        criterion: DiceCELoss(include_background=False, to_onehot_y=False, softmax=True)
            Loss function. Please NOTE: for classification, the loss function must convert `y` to OneHot.
            The default loss function applies to a multi-label target where the background class is omitted.

        metrics: DiceMetric(include_background=False, reduction=&#34;mean&#34;, get_not_nans=False)
            Metrics used for validation. Set to None to omit.

            The default metrics applies to a three-label target where the background (index = 0) class
            is omitted from calculation.

        val_metrics_transforms: None
            Post transform for the output of the forward pass in the validation step for metric calculation.

        predict_post_transforms: None
            Post transform for the output of the forward pass in the prediction step (only).

        learning_rate: float = 1e-2
            Learning rate for optimization.

        optimizer_class=AdamW
            Optimizer.

        num_res_units: int = 0
            Number of residual units for the UNet.

        dropout: float = 0.0
            Dropout ratio.
        &#34;&#34;&#34;

        super().__init__()

        self.criterion = criterion
        self.metrics = metrics
        self.learning_rate = learning_rate
        self.optimizer_class = optimizer_class
        self.val_metrics_transforms = val_metrics_transforms
        self.predict_post_transforms = predict_post_transforms
        self.net = MonaiUNet(
            spatial_dims=spatial_dims,
            in_channels=in_channels,
            out_channels=out_channels,
            channels=channels,
            strides=strides,
            num_res_units=num_res_units,
            dropout=dropout,
        )
        self.save_hyperparameters(ignore=[&#34;criterion&#34;, &#34;metrics&#34;])

    def configure_optimizers(self):
        &#34;&#34;&#34;Configure and return the optimizer.&#34;&#34;&#34;
        optimizer = self.optimizer_class(self.parameters(), lr=self.learning_rate)
        return optimizer

    def training_step(self, batch, batch_idx):
        &#34;&#34;&#34;Perform a training step.&#34;&#34;&#34;
        x, y = batch
        y_hat = self.net(x)
        loss = self.criterion(y_hat, y)
        self.log(&#34;train_loss&#34;, loss, on_step=False, on_epoch=True, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        &#34;&#34;&#34;Perform a validation step.&#34;&#34;&#34;
        x, y = batch
        y_hat = self.net(x)
        val_loss = self.criterion(y_hat, y)
        self.log(&#34;val_loss&#34;, torch.tensor([val_loss]), on_step=False, on_epoch=True, prog_bar=True)
        if self.metrics is not None:
            if self.val_metrics_transforms is not None:
                val_metrics = self.metrics(self.val_metrics_transforms(y_hat), y).mean()
            else:
                val_metrics = self.metrics(y_hat, y).mean()
            self.log(&#34;val_metrics&#34;, torch.tensor([val_metrics]), on_step=False, on_epoch=True)
        return val_loss

    def test_step(self, batch, batch_idx):
        &#34;&#34;&#34;Perform a test step.&#34;&#34;&#34;
        x, y = batch
        y_hat = self.net(x)
        test_loss = self.criterion(y_hat, y)
        self.log(&#34;test_loss&#34;, test_loss)
        return test_loss

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        &#34;&#34;&#34;The predict step creates a label image from the output one-hot tensor.&#34;&#34;&#34;
        x, _ = batch
        y_hat = self.net(x)
        if self.predict_post_transforms is not None:
            label = self.predict_post_transforms(y_hat).argmax(axis=1)
        else:
            label = y_hat.argmax(axis=1)
        return label

    def full_inference(
        self,
        data_loader: DataLoader,
        target_folder: Union[Path, str],
        inference_post_transforms: Transform,
        roi_size: Tuple[int, int],
        batch_size: int,
        overlap: float = 0.25,
        transpose: bool = True,
    ):
        &#34;&#34;&#34;Run inference on full images using given model.

        Parameters
        ----------

        data_loader: DataLoader
            DataLoader for the image files names to be predicted on.

        target_folder: Union[Path|str]
            Path to the folder where to store the predicted images.

        inference_post_transforms: Transform
            Composition of transforms to be applied to the result of the forward pass of the network.

        roi_size: Tuple[int, int]
            Size of the patch for the sliding window prediction. It must match the patch size during training.

        batch_size: int
            Number of parallel batches to run.
        
        overlap: float
            Fraction of overlap between rois.

        transpose: bool
            Whether the transpose the image before saving, to compensate for the default behavior of monai.transforms.LoadImage().

        Returns
        -------

        result: bool
            True if the inference was successful, False otherwise.
        &#34;&#34;&#34;

        # Make sure the target folder exists
        Path(target_folder).mkdir(parents=True, exist_ok=True)

        # Device
        device = &#34;cpu&#34;  # torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)

        # Switch to evaluation mode
        self.net.eval()

        # Process them
        c = 0
        with torch.no_grad():
            for images in data_loader:
                # Apply sliding inference over ROI size
                outputs = sliding_window_inference(
                    inputs=images,
                    roi_size=roi_size,
                    sw_batch_size=batch_size,
                    overlap=overlap,
                    predictor=self.net,
                    device=device,
                )

                # Apply post-transforms?
                outputs = inference_post_transforms(outputs)

                # Retrieve the image from the GPU (if needed)
                preds = outputs.cpu().numpy().squeeze()

                for pred in preds:
                    if transpose:
                        # Transpose to undo the effect of monai.transform.LoadImage(d)
                        pred = pred.T

                    # Save prediction image as tiff file
                    output_name = Path(target_folder) / f&#34;pred_{c:04}.tif&#34;
                    c += 1
                    with TiffWriter(output_name) as tif:
                        tif.write(pred)

                    # Inform
                    print(f&#34;Saved {output_name}.&#34;)

        print(f&#34;Prediction completed.&#34;)

        # Return success
        return True</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="qute.models.unet.UNet"><code class="flex name class">
<span>class <span class="ident">UNet</span></span>
<span>(</span><span>spatial_dims: int = 2, in_channels: int = 1, out_channels: int = 3, channels=(16, 32, 64), strides=(2, 2), criterion=DiceCELoss(
(dice): DiceLoss()
(cross_entropy): CrossEntropyLoss()
), metrics=&lt;monai.metrics.meandice.DiceMetric object&gt;, val_metrics_transforms=None, predict_post_transforms=None, learning_rate: float = 0.01, optimizer_class=torch.optim.adamw.AdamW, num_res_units: int = 0, dropout: float = 0.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrap MONAI's UNet architecture into a PyTorch Lightning module.</p>
<p>The default settings are compatible with a classification task, where
a single-channel input image is transformed into a three-class label image.</p>
<p>Constructor.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spatial_dims</code></strong> :&ensp;<code>int = 2</code></dt>
<dd>Whether 2D or 3D data.</dd>
<dt><strong><code>in_channels</code></strong> :&ensp;<code>int = 1</code></dt>
<dd>Number of input channels.</dd>
<dt><strong><code>out_channels</code></strong> :&ensp;<code>int = 3</code></dt>
<dd>Number of output channels (or labels, or classes)</dd>
<dt><strong><code>channels</code></strong> :&ensp;<code>tuple = (16, 32, 64)</code></dt>
<dd>Number of neuron per layer.</dd>
<dt><strong><code>strides</code></strong> :&ensp;<code>tuple = (2, 2)</code></dt>
<dd>Strides for down-sampling.</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>DiceCELoss(include_background=False, to_onehot_y=False, softmax=True)</code></dt>
<dd>Loss function. Please NOTE: for classification, the loss function must convert <code>y</code> to OneHot.
The default loss function applies to a multi-label target where the background class is omitted.</dd>
<dt><strong><code>metrics</code></strong> :&ensp;<code>DiceMetric(include_background=False, reduction="mean", get_not_nans=False)</code></dt>
<dd>
<p>Metrics used for validation. Set to None to omit.</p>
<p>The default metrics applies to a three-label target where the background (index = 0) class
is omitted from calculation.</p>
</dd>
<dt><strong><code>val_metrics_transforms</code></strong> :&ensp;<code>None</code></dt>
<dd>Post transform for the output of the forward pass in the validation step for metric calculation.</dd>
<dt><strong><code>predict_post_transforms</code></strong> :&ensp;<code>None</code></dt>
<dd>Post transform for the output of the forward pass in the prediction step (only).</dd>
<dt><strong><code>learning_rate</code></strong> :&ensp;<code>float = 1e-2</code></dt>
<dd>Learning rate for optimization.</dd>
</dl>
<p>optimizer_class=AdamW
Optimizer.</p>
<dl>
<dt><strong><code>num_res_units</code></strong> :&ensp;<code>int = 0</code></dt>
<dd>Number of residual units for the UNet.</dd>
<dt><strong><code>dropout</code></strong> :&ensp;<code>float = 0.0</code></dt>
<dd>Dropout ratio.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UNet(pl.LightningModule):
    &#34;&#34;&#34;Wrap MONAI&#39;s UNet architecture into a PyTorch Lightning module.

    The default settings are compatible with a classification task, where
    a single-channel input image is transformed into a three-class label image.
    &#34;&#34;&#34;

    def __init__(
        self,
        spatial_dims: int = 2,
        in_channels: int = 1,
        out_channels: int = 3,
        channels=(16, 32, 64),
        strides=(2, 2),
        criterion=DiceCELoss(include_background=False, to_onehot_y=False, softmax=True),
        metrics=DiceMetric(
            include_background=False, reduction=&#34;mean&#34;, get_not_nans=False
        ),
        val_metrics_transforms=None,
        predict_post_transforms=None,
        learning_rate: float = 1e-2,
        optimizer_class=AdamW,
        num_res_units: int = 0,
        dropout: float = 0.0,
    ):
        &#34;&#34;&#34;
        Constructor.

        Parameters
        ----------

        spatial_dims: int = 2
            Whether 2D or 3D data.

        in_channels: int = 1
            Number of input channels.

        out_channels: int = 3
            Number of output channels (or labels, or classes)

        channels: tuple = (16, 32, 64)
            Number of neuron per layer.

        strides: tuple = (2, 2)
            Strides for down-sampling.

        criterion: DiceCELoss(include_background=False, to_onehot_y=False, softmax=True)
            Loss function. Please NOTE: for classification, the loss function must convert `y` to OneHot.
            The default loss function applies to a multi-label target where the background class is omitted.

        metrics: DiceMetric(include_background=False, reduction=&#34;mean&#34;, get_not_nans=False)
            Metrics used for validation. Set to None to omit.

            The default metrics applies to a three-label target where the background (index = 0) class
            is omitted from calculation.

        val_metrics_transforms: None
            Post transform for the output of the forward pass in the validation step for metric calculation.

        predict_post_transforms: None
            Post transform for the output of the forward pass in the prediction step (only).

        learning_rate: float = 1e-2
            Learning rate for optimization.

        optimizer_class=AdamW
            Optimizer.

        num_res_units: int = 0
            Number of residual units for the UNet.

        dropout: float = 0.0
            Dropout ratio.
        &#34;&#34;&#34;

        super().__init__()

        self.criterion = criterion
        self.metrics = metrics
        self.learning_rate = learning_rate
        self.optimizer_class = optimizer_class
        self.val_metrics_transforms = val_metrics_transforms
        self.predict_post_transforms = predict_post_transforms
        self.net = MonaiUNet(
            spatial_dims=spatial_dims,
            in_channels=in_channels,
            out_channels=out_channels,
            channels=channels,
            strides=strides,
            num_res_units=num_res_units,
            dropout=dropout,
        )
        self.save_hyperparameters(ignore=[&#34;criterion&#34;, &#34;metrics&#34;])

    def configure_optimizers(self):
        &#34;&#34;&#34;Configure and return the optimizer.&#34;&#34;&#34;
        optimizer = self.optimizer_class(self.parameters(), lr=self.learning_rate)
        return optimizer

    def training_step(self, batch, batch_idx):
        &#34;&#34;&#34;Perform a training step.&#34;&#34;&#34;
        x, y = batch
        y_hat = self.net(x)
        loss = self.criterion(y_hat, y)
        self.log(&#34;train_loss&#34;, loss, on_step=False, on_epoch=True, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        &#34;&#34;&#34;Perform a validation step.&#34;&#34;&#34;
        x, y = batch
        y_hat = self.net(x)
        val_loss = self.criterion(y_hat, y)
        self.log(&#34;val_loss&#34;, torch.tensor([val_loss]), on_step=False, on_epoch=True, prog_bar=True)
        if self.metrics is not None:
            if self.val_metrics_transforms is not None:
                val_metrics = self.metrics(self.val_metrics_transforms(y_hat), y).mean()
            else:
                val_metrics = self.metrics(y_hat, y).mean()
            self.log(&#34;val_metrics&#34;, torch.tensor([val_metrics]), on_step=False, on_epoch=True)
        return val_loss

    def test_step(self, batch, batch_idx):
        &#34;&#34;&#34;Perform a test step.&#34;&#34;&#34;
        x, y = batch
        y_hat = self.net(x)
        test_loss = self.criterion(y_hat, y)
        self.log(&#34;test_loss&#34;, test_loss)
        return test_loss

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        &#34;&#34;&#34;The predict step creates a label image from the output one-hot tensor.&#34;&#34;&#34;
        x, _ = batch
        y_hat = self.net(x)
        if self.predict_post_transforms is not None:
            label = self.predict_post_transforms(y_hat).argmax(axis=1)
        else:
            label = y_hat.argmax(axis=1)
        return label

    def full_inference(
        self,
        data_loader: DataLoader,
        target_folder: Union[Path, str],
        inference_post_transforms: Transform,
        roi_size: Tuple[int, int],
        batch_size: int,
        overlap: float = 0.25,
        transpose: bool = True,
    ):
        &#34;&#34;&#34;Run inference on full images using given model.

        Parameters
        ----------

        data_loader: DataLoader
            DataLoader for the image files names to be predicted on.

        target_folder: Union[Path|str]
            Path to the folder where to store the predicted images.

        inference_post_transforms: Transform
            Composition of transforms to be applied to the result of the forward pass of the network.

        roi_size: Tuple[int, int]
            Size of the patch for the sliding window prediction. It must match the patch size during training.

        batch_size: int
            Number of parallel batches to run.
        
        overlap: float
            Fraction of overlap between rois.

        transpose: bool
            Whether the transpose the image before saving, to compensate for the default behavior of monai.transforms.LoadImage().

        Returns
        -------

        result: bool
            True if the inference was successful, False otherwise.
        &#34;&#34;&#34;

        # Make sure the target folder exists
        Path(target_folder).mkdir(parents=True, exist_ok=True)

        # Device
        device = &#34;cpu&#34;  # torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)

        # Switch to evaluation mode
        self.net.eval()

        # Process them
        c = 0
        with torch.no_grad():
            for images in data_loader:
                # Apply sliding inference over ROI size
                outputs = sliding_window_inference(
                    inputs=images,
                    roi_size=roi_size,
                    sw_batch_size=batch_size,
                    overlap=overlap,
                    predictor=self.net,
                    device=device,
                )

                # Apply post-transforms?
                outputs = inference_post_transforms(outputs)

                # Retrieve the image from the GPU (if needed)
                preds = outputs.cpu().numpy().squeeze()

                for pred in preds:
                    if transpose:
                        # Transpose to undo the effect of monai.transform.LoadImage(d)
                        pred = pred.T

                    # Save prediction image as tiff file
                    output_name = Path(target_folder) / f&#34;pred_{c:04}.tif&#34;
                    c += 1
                    with TiffWriter(output_name) as tif:
                        tif.write(pred)

                    # Inform
                    print(f&#34;Saved {output_name}.&#34;)

        print(f&#34;Prediction completed.&#34;)

        # Return success
        return True</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.module.LightningModule</li>
<li>lightning_fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="qute.models.unet.UNet.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Configure and return the optimizer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self):
    &#34;&#34;&#34;Configure and return the optimizer.&#34;&#34;&#34;
    optimizer = self.optimizer_class(self.parameters(), lr=self.learning_rate)
    return optimizer</code></pre>
</details>
</dd>
<dt id="qute.models.unet.UNet.full_inference"><code class="name flex">
<span>def <span class="ident">full_inference</span></span>(<span>self, data_loader: monai.data.dataloader.DataLoader, target_folder: Union[pathlib.Path, str], inference_post_transforms: monai.transforms.transform.Transform, roi_size: Tuple[int, int], batch_size: int, overlap: float = 0.25, transpose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Run inference on full images using given model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_loader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>DataLoader for the image files names to be predicted on.</dd>
<dt><strong><code>target_folder</code></strong> :&ensp;<code>Union[Path|str]</code></dt>
<dd>Path to the folder where to store the predicted images.</dd>
<dt><strong><code>inference_post_transforms</code></strong> :&ensp;<code>Transform</code></dt>
<dd>Composition of transforms to be applied to the result of the forward pass of the network.</dd>
<dt><strong><code>roi_size</code></strong> :&ensp;<code>Tuple[int, int]</code></dt>
<dd>Size of the patch for the sliding window prediction. It must match the patch size during training.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of parallel batches to run.</dd>
<dt><strong><code>overlap</code></strong> :&ensp;<code>float</code></dt>
<dd>Fraction of overlap between rois.</dd>
<dt><strong><code>transpose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the transpose the image before saving, to compensate for the default behavior of monai.transforms.LoadImage().</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code>bool</code></dt>
<dd>True if the inference was successful, False otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def full_inference(
    self,
    data_loader: DataLoader,
    target_folder: Union[Path, str],
    inference_post_transforms: Transform,
    roi_size: Tuple[int, int],
    batch_size: int,
    overlap: float = 0.25,
    transpose: bool = True,
):
    &#34;&#34;&#34;Run inference on full images using given model.

    Parameters
    ----------

    data_loader: DataLoader
        DataLoader for the image files names to be predicted on.

    target_folder: Union[Path|str]
        Path to the folder where to store the predicted images.

    inference_post_transforms: Transform
        Composition of transforms to be applied to the result of the forward pass of the network.

    roi_size: Tuple[int, int]
        Size of the patch for the sliding window prediction. It must match the patch size during training.

    batch_size: int
        Number of parallel batches to run.
    
    overlap: float
        Fraction of overlap between rois.

    transpose: bool
        Whether the transpose the image before saving, to compensate for the default behavior of monai.transforms.LoadImage().

    Returns
    -------

    result: bool
        True if the inference was successful, False otherwise.
    &#34;&#34;&#34;

    # Make sure the target folder exists
    Path(target_folder).mkdir(parents=True, exist_ok=True)

    # Device
    device = &#34;cpu&#34;  # torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)

    # Switch to evaluation mode
    self.net.eval()

    # Process them
    c = 0
    with torch.no_grad():
        for images in data_loader:
            # Apply sliding inference over ROI size
            outputs = sliding_window_inference(
                inputs=images,
                roi_size=roi_size,
                sw_batch_size=batch_size,
                overlap=overlap,
                predictor=self.net,
                device=device,
            )

            # Apply post-transforms?
            outputs = inference_post_transforms(outputs)

            # Retrieve the image from the GPU (if needed)
            preds = outputs.cpu().numpy().squeeze()

            for pred in preds:
                if transpose:
                    # Transpose to undo the effect of monai.transform.LoadImage(d)
                    pred = pred.T

                # Save prediction image as tiff file
                output_name = Path(target_folder) / f&#34;pred_{c:04}.tif&#34;
                c += 1
                with TiffWriter(output_name) as tif:
                    tif.write(pred)

                # Inform
                print(f&#34;Saved {output_name}.&#34;)

    print(f&#34;Prediction completed.&#34;)

    # Return success
    return True</code></pre>
</details>
</dd>
<dt id="qute.models.unet.UNet.predict_step"><code class="name flex">
<span>def <span class="ident">predict_step</span></span>(<span>self, batch, batch_idx, dataloader_idx=0)</span>
</code></dt>
<dd>
<div class="desc"><p>The predict step creates a label image from the output one-hot tensor.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_step(self, batch, batch_idx, dataloader_idx=0):
    &#34;&#34;&#34;The predict step creates a label image from the output one-hot tensor.&#34;&#34;&#34;
    x, _ = batch
    y_hat = self.net(x)
    if self.predict_post_transforms is not None:
        label = self.predict_post_transforms(y_hat).argmax(axis=1)
    else:
        label = y_hat.argmax(axis=1)
    return label</code></pre>
</details>
</dd>
<dt id="qute.models.unet.UNet.test_step"><code class="name flex">
<span>def <span class="ident">test_step</span></span>(<span>self, batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a test step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_step(self, batch, batch_idx):
    &#34;&#34;&#34;Perform a test step.&#34;&#34;&#34;
    x, y = batch
    y_hat = self.net(x)
    test_loss = self.criterion(y_hat, y)
    self.log(&#34;test_loss&#34;, test_loss)
    return test_loss</code></pre>
</details>
</dd>
<dt id="qute.models.unet.UNet.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a training step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, batch, batch_idx):
    &#34;&#34;&#34;Perform a training step.&#34;&#34;&#34;
    x, y = batch
    y_hat = self.net(x)
    loss = self.criterion(y_hat, y)
    self.log(&#34;train_loss&#34;, loss, on_step=False, on_epoch=True, prog_bar=True)
    return loss</code></pre>
</details>
</dd>
<dt id="qute.models.unet.UNet.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a validation step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, batch, batch_idx):
    &#34;&#34;&#34;Perform a validation step.&#34;&#34;&#34;
    x, y = batch
    y_hat = self.net(x)
    val_loss = self.criterion(y_hat, y)
    self.log(&#34;val_loss&#34;, torch.tensor([val_loss]), on_step=False, on_epoch=True, prog_bar=True)
    if self.metrics is not None:
        if self.val_metrics_transforms is not None:
            val_metrics = self.metrics(self.val_metrics_transforms(y_hat), y).mean()
        else:
            val_metrics = self.metrics(y_hat, y).mean()
        self.log(&#34;val_metrics&#34;, torch.tensor([val_metrics]), on_step=False, on_epoch=True)
    return val_loss</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="qute.models" href="index.html">qute.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="qute.models.unet.UNet" href="#qute.models.unet.UNet">UNet</a></code></h4>
<ul class="">
<li><code><a title="qute.models.unet.UNet.configure_optimizers" href="#qute.models.unet.UNet.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="qute.models.unet.UNet.full_inference" href="#qute.models.unet.UNet.full_inference">full_inference</a></code></li>
<li><code><a title="qute.models.unet.UNet.predict_step" href="#qute.models.unet.UNet.predict_step">predict_step</a></code></li>
<li><code><a title="qute.models.unet.UNet.test_step" href="#qute.models.unet.UNet.test_step">test_step</a></code></li>
<li><code><a title="qute.models.unet.UNet.training_step" href="#qute.models.unet.UNet.training_step">training_step</a></code></li>
<li><code><a title="qute.models.unet.UNet.validation_step" href="#qute.models.unet.UNet.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>